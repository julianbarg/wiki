**Mission Improbable. Using Fantasy Documents to Tame Disaster**

[[_TOC_]]

## Citation

Clarke, L. (1999). *Mission Improbable. Using Fantasy Documents to Tame Disaster*. Chicago: The University of Chicago Press. Retrieved from [[https://press.uchicago.edu/ucp/books/book/chicago/M/bo3645893.html]]

## One-paragraph summary

The plans for responding to an oil spill in Alaska were never written with an actual emergency response in mind. Rather, they were rhetorical devices, to convince other organizations of the viability of the pipeline. However, then these plans collided with reality.

### Methods

## Related to

### Theories

### Conferences, courses etc.

### Previous literature/influences

### Projects

### Cited by

## Concepts

## Resources

## Chapter-by-chapter summary

### Chapter 1: Some Functions of Planning

(1) Planning is rhetorical. (2) Planning is an undertaking of organizations. For instance, only organizations like the coast guard can mobilize the incredible responses that are utilized in spill response. Or also self-similarity: organizations can "splinter", e.g., form sub-units, and those sub-units will look alike. And organizations have complex memory--"stories, memos, official histories, and institutionalized patterns of behavior" (p. 7). In case of complex disasters, planning is more an excercise in appearing rational, and therefore legitimate, though. (3) Organizations, in the case of complex challenges, do not know what matters (I guess). Therefore, there are some limits to planning. (4) Laying out the difference between risk and uncertainty. "[R]isk is when you know the possible range of things that may happen following a choice; uncertainty is when you don't" (p. 11). Organizations attempt to translate uncertainties into risk. Distinguishes between operational utility (rational planning) and symbolic utility. (5) Lots of stuff? Planning is political?! "[S]ymbolic plans, which I call fantasy docuemnts, are rhetocial instruments that have poltical utility in reducing uncertainty for organizations and experts" (p. 13). These "fantasy documents" are mostly there to categorize problems, assign them to existing structure (and thus indicate who might be in charge of an evolving situation).

### Chapter 2: Fantasy Documents

Some examples of excercises and how they expose the flaws of fantasy documents. "[Fantasy documents" are tested against reality only rarely" (p. 30). "An important part of the story I want to tell has to do with how people and organizations become so certain they know what the future will entail" (p. 40). "The fantasies can lead people working within high-risk orgnaizations to be overconfident that their procedures are strong enough to prevent system breakdown; and they can lead people outside organizations to believe promises that their interests are protected" (p. 42).

### Chapter 3: Planning and the C-Shibboleths

Communication, cooperation, coordination. "Often enough, planning and success *do not* coincide, but are loosely connected or even decoupled" (p. 57; emphasis in original). Important keyword: worst case scenario. "[C]orrespondence between plans and successful response is often illusory. That correspondence is often more something we'd *like* to believe is there than something real. For to admit the observe--that planning may have little to do with successful evacuation--is unnerving at best and at worst undermines the main supporting beam of Western rationalism: control based on expert knowledge" (pp. 68f; emphasis in original).

### Chapter 4: Apparent Affinities

We have to have plans, because it would be unthinkable just to respond "we cannot plan for that" (p. 71). Instead, organizations reach for affinities and try to turn them into God-terms that are accepted by outsiders (and insiders) without questioning (p. 73). The plan for Alaska oil recovery was to recover 95% of oil spilled (p. 79), based on ideal conditions and a smaller spill. The plan for the Long Island nuclear reactor war to create an affinity to natural disasters (p. 88). Organizations also wield these affinities by creating symbols for the problem at hand out of other events, thus gaining symbolic control of the challenge (p. 99).

### Chapter 5: Authority and Audience in Accepting Risk

Conflict over defining what is "normal". To make a successful claim, one needs to prove expertise, not validity (since validity in this case cannot be proven). Also describes regulatory capture. And describes the same process of split view as I have (p. 11).

In the case of civil defense, there is a very similar phenomenon to what I am studying. The organization, in this case part of the state, needed to convince the public and other organization (on the local level) of their plans, because their cooperation was necessary. They created fantasy documents, under the banner of dual-hazards planning. This caused the launch of FEMA, the original organizaztion ceased to exist. Did they start to believe in their own fantasy documents (pp. 124f). A specific strategy is not only to produce the esoteric knowledge (or the appearance thereof), but to also find a way to challenge others' expertise in the area (p. 129). Clarke claims that there is very little actual data in the fantasy documents. In the case of oil spills, existing regulations "could be referred to as authoritative knowledge" (p. 130)--even though actual real-life experience with spill of the relevant category did not exist at that time.

The most important battles however were carried out leading up to the decision of who would be respected as an expert (pp. 134f).

### Chapter 6: Organizations, Symbols, Publics

It is possible that the involved organizations don't even know they are misconstruing the truth (p. 141). "[T]he chasm between what officials do and what they say comes from their need or desire to protect their interests, shielding what they are really up to from outside scrutiny" (p. 145). The purpose of politicised language is to gain support. "In other words, action organizations actually produce useful things but political organizations produce rhetoric, in response to outside pressures" (p. 146). Clarke challenges the assessment of organizational efforts as conspiracy theories however. The process of coming up with fantasy document-type rhetorics must not be centrally planned, but rather emerge through an organizational process (p. 146). Weick's concept of enactment is one avenue to explain the process. Later he raises the point that risk assessment is inherently political (under conditions of uncertainty)--therefore, experts are also political actors, and to say that they are just neutral assessors or risk is untrue (p. 158).

Basically later arguing that social structure leads to assuming risk (pp. 164f).

"To admit you can't do a thing to contain huge amounts of oil on open waters leads to a discussion about safer technology, conservation, and alternative energy sources" (p. 178).

## Important points

* Nuclear "raid" excercise in 1955--nobody reacts (p. 121).
* "Beliving in the plans was part of believing in their jobs" (p. 142).
* "There is an imperative of action attached to managerial positions: if leaders do not convince others that they are leading, or trying to lead, their environments are likely to select them out" (p. 151).
* "The problem is that failures and accidents aren't special" (p. 152).
* Expertise has come to be seen as inherently *good* in post-industrial societies that emphasize the importance of knowledge (p. 157).
* "For neoclassical economics, when people don't act in accordance with the theory economists are more likely to conclude that something is wrong with the people than the theory" (p. 162).
* Arguing about actual, applied experience being necessary for developing successful new technology (p. 163).
* If fantasy documents make us feel at ease, they increase the risk (p. 167).

## Reactions

* If we had serious discussions, only about what we can really have experience of, we will be more likely to actually address issues. That includes forsaking the notion of real control over unknowns (p. 132).
* I find it difficult to draw the difference between planning for nuclear war, and the extrapolations that are routinely done in quantitative work in our area (p. 133).
* It is conceivable that the operator of the nuclear power station is actually convinced of her own hypothesis--that the populace will follow the evacuation plan in the case of a real emergency. Maybe here I disagree with Clarke?
* So experts predict--and Clarke's criticism comes into play because they predict the unpredictable (Chapter 6). But what if what happens is entirely predictable, and yet experts still refuse to reign in it? As is my case of onshore pipeline spills.
* Do I really want to say that organizations should not operate steam engines, or that the development of steam engines was a mistake (p. 163)?
* A problem is if experts knowledge never gets tried by reality (p. 164).